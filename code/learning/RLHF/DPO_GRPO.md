强化学习在大语言模型山的重要作用：

1. 强化学习比SFT更可以考虑整体影响：SFT针对单个token进行反馈，其目标是要求模型针对给定的输入给出确切答案。而强化学习是针对整个输出文本进行反馈，并不针对特定的token，这种反馈粒度的不同，使得强化学习更适合大语言模型，既可以兼顾表达多样性，还可以增强对微小变化的敏感性。自然语言十分灵活，可以用多种不同的方式表达相同的语义，而有监督学习很难支持上述学习方式。

2. 强化学习更容易解决幻觉问题：用户在大语言模型是主要由三类输入：

   a. 文本型：用户输入相关文本和问题，让模型基于所提供的文本生成答案；

   b. 求知型：用户仅提出问题，模型根据内在知识提供真实回答；

   c. 创造型：用户为提供问题或说明，让模型进行创造性输出。SFT非常容易使得求知型query产生幻觉。在模型并不包含或者知道答案的情况下，SFT仍然会促使模型给出答案。而使用强化学习方法，则可以通过定制奖励函数，将正确答案赋予非常高的分数，放弃回答的答案赋予中低分数，不正确的答案赋予非常高的负数，使得模型学会依赖内部知识选择放弃回答，从而在一定程度上缓解模型幻觉问题。

3. 强化学习可以更好的解决多轮对话奖励累积问题：多轮对话能力是大语言模型重要的基础能力之一，多轮对话是否达成最终目标，需要考虑多次交互过程的整体情况，因此很难使用SFT方法构建，而使用强化学习方法，可以通过构建奖励函数，将当前输出考虑整个对话的背景和连贯性。



## 一、DPO

> DPO算法远比PPO算法训练难度低，PPO算法对资源消耗很大
>
> PPO算法会多次采样，标注样本并不是只有好与不好两种样本，而是会存在多个样本，会有打分排序，还会引入奖励模型（reward model）

![img](https://pic2.zhimg.com/v2-a2cf5f95d8fa7c50a677882455df20ef_r.jpg)

DPO可以直接依据策略来定义偏好损失，当存在一个关于模型响应的人类偏好数据集时，DPO能够在训练过程中，使用简单的二元交叉熵目标来对策略进行优化，而无需明确地去学习奖励函数或者从策略中进行采样。

![img](https://pic4.zhimg.com/v2-28c7e1f000e6447a36d8c79daf2747ab_r.jpg)

DPO算法的优化目标更为简单，利用了从奖励函数到最优策略的解析映射，允许直接使用人类偏好数据进行简化的优化过程

该目标增加了对偏好数据 $y_w$ 可能性，并减少非偏好$y_l$ 可能性

![img](https://pic2.zhimg.com/v2-cc81e93e0d9405972ed2714e350ed23b_r.jpg)

DPO的数据集主要是三部分组成：instruct prompt、chosen、rejected

DPO是一种直接优化人类偏好的语言模型训练方法，通过简化传统RLHF流程来提升效率。以下从优缺点两个维度展开分析：

**DPO核心优势：**

1. 训练效率显著提升

- 省去奖励模型训练：DPO绕过了RLHF中独立的奖励模型训练阶段，直接利用偏好数据调整模型策略，减少了计算成本和训练时间
- 单阶段优化：将问题转换为分类任务（最大化偏好数据似然），无需交替优化策略和奖励模型，简化了实现流程

2. 算法稳定性更强

- 规避强化学习的高方差问题：传统RLHF依赖策略梯度方法，容易因奖励稀疏或噪声导致训练不稳定，而DPO通过概率匹配直接优化策略，降低方法。
- 数学理论保障：DPO基于偏好模型的严格数学推导，确保在理想数据下收敛到最优策略

3. 实现复杂度低

- 无需复杂调参：RLHF需平衡测策略更新幅度，奖励模型置信度等超参数，DPO仅需调整学习率和偏好权重，更易工程化。

4. 数据利用更高效

- 直接偏好建模：通过对比正/负样本对的偏好概率，更精准地捕捉人类意图，避免奖励模型因“过度简化”偏好关系导致的偏差。

**DPO的局限性：**

1. 对数据质量高度敏感

- 依赖高质量偏好数据：若数据存在噪声或覆盖不全，模型易过拟合错误偏好。RLHF可通过奖励模型泛化部分噪声，单DPO直接又来原始数据。
- 需大规模标注数据：DPO的性能与偏好数据量强相关，标注成本可能成为瓶颈

2. 灵活性受限

- 静态偏好假设：DPO假设偏好关系固定，难以动态适应复杂或分层的偏好
- 难以引入先验知识：RLHF允许手动调整奖励模型，而DPO完全由数据驱动，缺乏显示干预接口

3. 可解释性较弱

- 黑箱优化过程：DPO直接调整策略参数，缺乏RLHF中奖励模型的中间信号，难分析模型决策依据，不利于调试

4. 扩展性挑战







参考：

[大模型优化利器：RLHF之PPO、DPO](https://www.zhihu.com/tardis/bd/art/717010380)

































